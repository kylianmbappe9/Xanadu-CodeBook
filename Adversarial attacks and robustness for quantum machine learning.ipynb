{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is adversarial attack and why is it dangerous?\n",
    "\n",
    "#Adversarial attacks are small, carefully crafted perturbations to input data that lead to high rates of misclassification for machine learning models, while the data seems to be identical to the original for human observers.\n",
    "\n",
    "#hink of it like twins. One has medium sized hair and another does have medium sized hair owever, the length may differ. One gets blamed for something he didnt do even though when he recieves that information, he wont know what in the world is he listening so the final judgement output would be blamed on him, which is wrong. Thesse mistakes are extremely dangerous ESPECIALLY IN THE MEDICAL or CYBERSECURITY sector. I may be wrong but lets say that when they detect a tumor (Gliomal, Meningliomal or pituitary), they may identify a different tymor WHY? because of that little mishap along the way so the patient will be treated differently even though that tumor could be differnt OR not a tumor at all. Even for cyber secuirty, something which can be considered a SCAM, due to this adversible attack, the QC may pressume it is not something fishy or stuff and will actually place it through (transactions are core here)...   \n",
    "\n",
    "#how can we prevent such adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1) we first import the necessary modules \n",
    "\n",
    "my necessary modules are numpy, matplotlb, torch, pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are [PlusMinus] data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plus minus data set consists for grayscale 16 x 16 images which consists of \n",
    "\n",
    "#1) + [plus dataset]\n",
    "#2) - [minus dataset]\n",
    "#3) ⊢ [horizontal T data set]\n",
    "#4) ⊣ [mirror horizaontal T data set]\n",
    "\n",
    "these data sets are mostly used in QML models whcih are scalabable, (low dimension images) something which the QPU can process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QML image classification model\n",
    "\n",
    "#QML image classification model, builds a classical model (quantum model) that can accurately classify the input image which the quantum circuit has retrieved. While the QML model is to be trained, one thing that must be acknowledged is the presence of adversarial attacks when undergoing quantum processing of a specific input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets import the already haved images inside the pennylane dataset\n",
    "\n",
    "#A QUICKL REMINER, TO MAKE USE OF PLUS-MINUS DATA SETS\n",
    "#YOU WILL NEED TO DOWNLOAD aiohttp, h5py and fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "[pm] = qml.data.load('other', name='plus-minus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#so we have several train and test data sets which are already included in the plus-minus data set imported by pennylane. Here out of all the iamges, our X axis will be the iamges, while the Y axis will be the labels, respectively segregated each axis into training data set and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pm.img_train\n",
    "X_test = pm.img_test\n",
    "Y_train = pm.labels_train\n",
    "Y_test = pm.labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#now randomly show an image that are assigned to each axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vis = [\n",
    "    (X_train[Y_train == 0])[0],\n",
    "    (X_train[Y_train == 1])[1],\n",
    "    (X_train[Y_train == 2])[2],\n",
    "    (X_train[Y_train == 3])[3],\n",
    "]\n",
    "y_vis = [0, 1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1) creating a visualized function with my input imagfes, the label for those input images and (optional: whhether prediction label can be none or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2) i am setting the length of those images as a variable n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3) u2212 denotes -, u002b denotes +, ua714 denotes ⊢, u02e7 denotes ⊣\n",
    "why do we need that, on the previous code we have each image for Xtrain, Xtest, Ytrain, Ytest. From all those iamges, instead of assigning each images as [image 0 or image 1 or image 2 or image 3], we will simply set those images as either + or - or ⊣ or ⊢, how? the computer system will look at the image and analyse how does that graph look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4) so for step 4, because we have 4 images to display we would typicallyt create a table where row 1 column1 has one image, row 1 column 2 has one image row 1 column 3 has one image and row 1 column 4 has one iamge. Each of those images are 8 inches wide and 2 inches tal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5) we want to go through every image that we have input or assigned at the moment so we make use of a for loop to go thorugh all the iamges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) for every images that we are going through, first we open one image from the axes data set, open the iamge, convert hte image to grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7) for every image, if the prediction label is set to none, then for that specific iamge [ASSINGED IN X], we assign the true label [ASSIGNED IN Y IN CORREALTION TO X], else for that specific iamge [ASSINGED IN X], we assign the true label [ASSIGNED IN Y IN CORREALTION TO X] and the predicted label [PREDICTING ASSIGNED LABEL IN Y IN CORREALTION TO X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8) space has been added between two iamges so as to not get clustered because if it clustered, it would be harder to read and harder to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISUALIZING THE DATA SET\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAADRCAYAAABVTvQLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdRUlEQVR4nO3df2xV9f3H8ddtoZcCpa4CLZfxU1FkQdxg7TBlwKiWZiPrNjclWwRmWGbQzRElsgAFRtLI5tYxOzEzyogLUxchy6YsG9K5ZFB+CCoEtLASitAKVegPaEvb8/2DL3er3NvP6em5955z7/ORnISe++45n3vuefX0zem9n4BlWZYAAAAAwIG0RA8AAAAAgH/RUAAAAABwjIYCAAAAgGM0FAAAAAAco6EAAAAA4BgNBQAAAADHaCgAAAAAOEZDAQAAAMAxGgoAAAAAjtFQJMipU6cUCAT0i1/8wrVtVlVVKRAIqKqqyrVtAolAPgAzcgJERjbij4aiD7Zs2aJAIKADBw4keiiA55APwIycAJGRDX+joQAAAK549913tXfv3kQPA/C88ePHJ9XdDhoKAHAoEAhoy5YtiR4G4Bmvvfaann/++UQPA0Cc0VC4rKOjQ2vWrNH06dOVnZ2tIUOGaNasWdq9e3fU7/nVr36lcePGKTMzU7Nnz9aRI0duqDl+/Ljuu+8+5eTkaNCgQZoxY4b+/Oc/G8dz+fJlHT9+XBcuXOjX8wLcQD4AM3ICREY2vIuGwmVNTU16/vnnNWfOHD311FNau3atzp8/r+LiYh0+fPiG+q1bt2rTpk1atmyZVq5cqSNHjugrX/mKGhoawjVHjx7Vl770JR07dkxPPvmknn76aQ0ZMkSlpaXavn17r+PZt2+f7rjjDj3zzDNuP1Wgz8gHYEZOgMjIhodZsO3FF1+0JFn79++PWtPZ2Wm1t7f3WPfJJ59Yubm51ve///3wutraWkuSlZmZaZ05cya8vrq62pJk/eQnPwmvmzdvnjV16lSrra0tvK67u9u6++67rUmTJoXX7d6925Jk7d69+4Z1ZWVlTp4yYFsq5kOS9eKLLzr6XqSmZM9JWVmZ9dBDDxnrgE9L9my0trZa58+fDy9jxozpsS2/4w6Fy9LT05WRkSFJ6u7u1scff6zOzk7NmDFDb7/99g31paWlGj16dPjr/Px8FRQU6PXXX5ckffzxx3rzzTf1ne98R83Nzbpw4YIuXLigxsZGFRcXq6amRh9++GHU8cyZM0eWZWnt2rXGsXd2dqqtrc24tLe39/GoANf4OR+XL18Ob//6IkktLS091n3yySd9OSTADfycE0k6d+6cqqqqwsulS5f68OyB6PycjY0bN2rEiBHhpa6uro/P3tsGJHoAyej3v/+9nn76aR0/flxXr14Nr58wYcINtZMmTbph3W233aZXXnlFknTixAlZlqXVq1dr9erVEff30Ucf9QiMUxs2bNC6deuMdbm5uaqvr+/3/pCa/JqPjRs3RszHo48+qkcffTT89bhx43Tq1Kl+7w+pza85ka59Xv8777wT/nr79u364he/6Mq2Ab9m48EHH1RhYWH46+9973v93qaX0FC47KWXXtLixYtVWlqqJ554QiNHjlR6errKy8t18uTJPm+vu7tbkvT444+ruLg4Ys2tt97arzFfV1paqvHjxxvrMjMzXdkfUo+f8/Hpi4Ek3XPPPXriiSd07733hteRD/SXn3MiSQsXLuSTnhATfs7GxIkTNXHixPDXgwYNcmW7XkFD4bI//elPmjhxol577TUFAoHw+rKysoj1NTU1N6z74IMPwr/YXz/5Bg4cqKKiIvcH/D/uuusu3XXXXTHdB1Kbn/Px6YvBdVOmTIn5vpFa/JwTIJbIhnfxHgqXpaenS5Isywqvq66u1p49eyLW79ixo8ff5+3bt0/V1dUqKSmRJI0cOVJz5szRc889p3Pnzt3w/efPn+91PHykGbyEfABm5ASIjGx4F3coHHjhhRe0c+fOG9b/+Mc/1te+9jW99tpr+sY3vqGvfvWrqq2t1ebNmzVlyhS1tLTc8D233nqrCgsL9fDDD6u9vV0VFRW6+eabtWLFinBNZWWlCgsLNXXqVC1dulQTJ05UQ0OD9uzZozNnzvT4W9VP27dvn+bOnauysjLbb6gD+oN8AGbkBIiMbPgTDYUDzz77bMT1ixcv1uLFi1VfX6/nnntOf/vb3zRlyhS99NJLevXVVyNOsf7ggw8qLS1NFRUV+uijj5Sfn69nnnlGo0aNCtdMmTJFBw4c0Lp167RlyxY1NjZq5MiR+vznP681a9bE6mkCjpAPwIycAJGRDX8KWP973wgAAAAA+oD3UAAAAABwjIYCAAAAgGM0FAAAAAAco6EAAAAA4BgNBQAAAADHaCgAAAAAOOa5eSi6u7t19uxZZWVl9ZhWHbDDsiw1NzcrFAopLS35+mXyAafIBhBZsmdDIh9wznY+rBh55plnrHHjxlnBYNDKz8+3qqurbX1fXV2dJYmFpV9LXV1drE7tfnOaDcsiHyz9X7ycDcvi2sGSuCVZs2FZ5IOl/4spHzG5Q/Hyyy9r+fLl2rx5swoKClRRUaHi4mK9//77GjlyZK/fm5WVFYshIcV49TzqTzYk7z4vN+Tl5Rlrli5d6sq+fve73xlr6uvrXdmX13j5HPLTtWPIkCHGmmXLlhlr7r//fmNNdXW1sWb9+vXGmmQ9p92SrNmQvP3c+mvy5MnGmsrKSlf2ZSfTx48fd2VfXmM6h2LSUPzyl7/U0qVLtWTJEknS5s2b9de//lUvvPCCnnzyyV6/l1txcINXz6P+ZEPy7vNyg50/NRg0aFDc9pWsvHwO+enaYWd/wWDQWDN06FBjTWZmprEmlc9ptyRrNiRvP7f+Sk9PN9bY+Q8At/aVrEznkOs/gTo6OnTw4EEVFRX9dydpaSoqKtKePXvc3h3gG2QDiI58AJGRDfiB63coLly4oK6uLuXm5vZYn5ubG/E2UHt7u9rb28NfNzU1uT0kwBP6mg2JfCB1cO0AIuPaAT9I+D3S8vJyZWdnh5cxY8YkekiAZ5APIDKyAURHPhBvrjcUw4cPV3p6uhoaGnqsb2hoiPimy5UrV+rSpUvhpa6uzu0hAZ7Q12xI5AOpg2sHEBnXDviB6w1FRkaGpk+frl27doXXdXd3a9euXZo5c+YN9cFgUMOGDeuxAMmor9mQyAdSB9cOIDKuHfCDmHzK0/Lly7Vo0SLNmDFD+fn5qqioUGtra/jTCYBURTais/MpNYMHD3ZlO3wijjf5KR9Xrlwx1vznP/8x1mRnZxtrCgsLjTW33367sebs2bPGGniTn7IRb3Y+wcrOpzxZluXKvlJVTBqK+++/X+fPn9eaNWtUX1+vu+66Szt37rzhDUVAqiEbQHTkA4iMbMDrYtJQSNIjjzyiRx55JFabB3yLbADRkQ8gMrIBL+O+PwAAAADHaCgAAAAAOEZDAQAAAMAxGgoAAAAAjtFQAAAAAHCMhgIAAACAYzH72FgASITu7u5EDwEpoKury1hz/vx5Y01bW5sbw+G8R8qyMyFdR0eHsWbgwIHGmgED+LU5Gu5QAAAAAHCMhgIAAACAYzQUAAAAAByjoQAAAADgGA0FAAAAAMdoKAAAAAA4RkMBAAAAwDEaCgAAAACOMUMHAAAxcPXqVWONnQm37EymlZmZ6cp2Ojs7jTWAl9TW1hprdu/ebaxZunSpsaawsNBY89577xlr7EyM6TfcoQAAAADgGA0FAAAAAMdoKAAAAAA4RkMBAAAAwDEaCgAAAACO0VAAAAAAcIyGAgAAAIBjNBQAAAAAHGNiOwCe0N3dbaxJSzP/H4idCYOScVIheM/Ro0eNNfv37zfWfPOb3zTWzJo1y1hTVVVlrHFrYrtAIGCs+cxnPuPKdrKzs401oVAo6mOdnZ3au3evcRvwJjvn7JUrV4w1AwcONNYEg0FbY0pF3KEAAAAA4BgNBQAAAADHaCgAAAAAOEZDAQAAAMAxGgoAAAAAjtFQAAAAAHCMhgIAAACAYzQUAAAAABzz7MR2JSUlvU4ykpmZadzGgQMHjDWtra3GGjsT69ipuXDhgrGmo6PDlX3ZYVmWK9tBYrh1HsTD0KFDjTUZGRnGmrNnzxprCgoKjDVf+MIXjDV2JiUbN26csSaeOettwr7Ozk5VV1fHbSywN1mjnZr09HRjjZ1J4kaPHm2sGTt2rLEmNzfXWGPn59PcuXONNYMGDTLW3HTTTcaam2++Oepjra2tuvfee43bgDfF83ciO3lNVa7foVi7dq0CgUCPZfLkyW7vBvAdsgFERz6AyMgG/CAmdyg+97nP6R//+Md/dzLAszdCgLgiG0B05AOIjGzA62JyRg4YMEB5eXmx2DTga2QDiI58AJGRDXhdTN6UXVNTo1AopIkTJ+q73/2uTp8+HbW2vb1dTU1NPRYgWfUlGxL5QGrh2gFExrUDXud6Q1FQUKAtW7Zo586devbZZ1VbW6tZs2apubk5Yn15ebmys7PDy5gxY9weEuAJfc2GRD6QOrh2AJFx7YAfuN5QlJSU6Nvf/rbuvPNOFRcX6/XXX9fFixf1yiuvRKxfuXKlLl26FF7q6urcHhLgCX3NhkQ+kDq4dgCRce2AH8T8XT033XSTbrvtNp04cSLi48FgUMFgMNbDADzHlA2JfCB1ce0AIuPaAS+K+cR2LS0tOnnypEaNGhXrXQG+QjaA6MgHEBnZgBe5fofi8ccf14IFCzRu3DidPXtWZWVlSk9P18KFC/u0nd/+9rfKysqK+ridzvvQoUPGGrcmtktLM/dmb775prGmsbHRlX3Zqdm/f7+xxs5kfHY+vq6zs9NYY2fSst4m7pKuTUzj1Yln3MqGdG1CqN6Ou51J4txiek2kax95aDJixAhjzfTp0401d999t7HGzmRZNTU1xppQKGSsiefEdr298fLKlSuentjOzXx4RXt7u7HmzJkzrmxn/vz5xho7f0d/yy23GGtGjhxprLHzc7i3a/x1dq5ldq7RJ0+ejPpYPH9eOpGM2XCTV6/5qcb1huLMmTNauHChGhsbNWLECBUWFmrv3r22flkAkhnZAKIjH0BkZAN+4HpD8cc//tHtTQJJgWwA0ZEPIDKyAT+I+XsoAAAAACQvGgoAAAAAjtFQAAAAAHCMhgIAAACAYzQUAAAAAByjoQAAAADgmOsfG+uWDz/8UEOHDo36uJ2JqfLz8401dibIszMxlZ2a2bNnG2vi6f333zfWtLS0GGvS09ONNb1NuHWdnYn/TJOotbe369e//rVxO3533333KTMzM+rj2dnZxm24NRmQnYmn7Lz+diY/zMnJMdYMHjzYWHPzzTcba44ePWqs2blzp7HGzqRbdtg5zr1N1Hn16lVXxgH7mpubjTUHDhxwZTsTJkww1nz2s5811pw+fdqVGjs/X/79738bay5fvmyssZONf/3rX1EfS6VsDB06tN8/k+wc70mTJhlr7EyKa4ed3wfHjx9vrLHzuwyi4w4FAAAAAMdoKAAAAAA4RkMBAAAAwDEaCgAAAACO0VAAAAAAcIyGAgAAAIBjNBQAAAAAHKOhAAAAAOCYZye2++EPf9jrJCN2JmaZNWuWsaa3yfOuszNBj52JxOyMZ+DAga6MZ8iQIcaayZMnG2vcYuf1KioqMta0tbX1+nhTU1NKTGz3xhtv9HquuDVhox12Jjn64IMPjDV2Ju9asGCBsebSpUvGmldffdVY85e//MVYU19fb6yxc3ziwa3XG+6y8zPfzgRgZ86cMdZs377dWPPGG28Ya86dO2essTNmO9v5+OOPjTV2MtbbBHmplI1Vq1b1OhFcRkaGcRt2JoCz8/tOb5OzXmfntbHz+8Xo0aONNXaymErnSl9540oHAAAAwJdoKAAAAAA4RkMBAAAAwDEaCgAAAACO0VAAAAAAcIyGAgAAAIBjNBQAAAAAHKOhAAAAAOCYZye2O3LkSL+38d577xlr7EyIYmciucGDBxtr8vLyjDV2Jk3p6uoy1uTk5Bhr8vPzXRmPnZphw4YZa+655x5jTUdHR6+P9zZ5UTJ54403bJ27XtHZ2WmssfN87Eze1djYaKypra011tTV1RlrgP6yMxGjnfM+FAoZa959911jzd///ndjjZ08w5vuu+8+ZWVlRX3czmS/V69eNdYcO3bMWHP+/HljjR12Jjbs7TlfZ+f3Jj9dd+ONOxQAAAAAHKOhAAAAAOAYDQUAAAAAx2goAAAAADhGQwEAAADAMRoKAAAAAI7RUAAAAABwjIYCAAAAgGOendjODXYmgHNLS0uLsebEiRNxGMk1p06dMtYcOnTIlX3Zmdhu4MCBxpo//OEPxhrTJIN2JiFMBvE8t+PFzoRBdiYBM01+KEkjRoww1gSDQWNNe3u7sQbojZ3J5k6fPm2smTx5srFm+PDhxhom7kpuP/3pT5WRkRH1cTvXajvXn+rqamONWz8/Bw0aZKz50Y9+ZKxZsmSJG8NJWX2+Q/HWW29pwYIFCoVCCgQC2rFjR4/HLcvSmjVrNGrUKGVmZqqoqEg1NTVujRfwLLIBREY2gOjIB5JBnxuK1tZWTZs2TZWVlREf37hxozZt2qTNmzerurpaQ4YMUXFxsdra2vo9WMDLyAYQGdkAoiMfSAZ9/pOnkpISlZSURHzMsixVVFRo1apV+vrXvy5J2rp1q3Jzc7Vjxw498MAD/Rst4GFkA4iMbADRkQ8kA1fflF1bW6v6+noVFRWF12VnZ6ugoEB79uyJ+D3t7e1qamrqsQDJxkk2JPKB5Ec2gOjIB/zC1Yaivr5ekpSbm9tjfW5ubvixTysvL1d2dnZ4GTNmjJtDAjzBSTYk8oHkRzaA6MgH/CLhHxu7cuVKXbp0KbzU1dUlekiAZ5APIDKyAURHPhBvrjYUeXl5kqSGhoYe6xsaGsKPfVowGNSwYcN6LECycZINiXwg+ZENIDryAb9wtaGYMGGC8vLytGvXrvC6pqYmVVdXa+bMmW7uCvAVsgFERjaA6MgH/KLPn/LU0tLSY4K22tpaHT58WDk5ORo7dqwee+wxbdiwQZMmTdKECRO0evVqhUIhlZaWujluuMDOhHRuuXr1qrHm3LlzcRhJ7JCN/rEzKaGdz163M8nkHXfcYazJysoy1jCxnT1kI7q0NPP/69mZbMwOO5PW2RkP3BXPfLzyyitxmbwwnr9f2MlHY2OjsSZVJsaNlT43FAcOHNDcuXPDXy9fvlyStGjRIm3ZskUrVqxQa2urfvCDH+jixYsqLCzUzp07bc1kCPgZ2QAiIxtAdOQDyaDPDcWcOXN67TwDgYDWr1+v9evX92tggN+QDSAysgFERz6QDLi3CQAAAMAxGgoAAAAAjtFQAAAAAHCMhgIAAACAYzQUAAAAAByjoQAAAADgWJ8/NhYAEsWtCZniOekS0JumpiZjzbFjx4w1X/7yl401s2fPNta8+uqrxpr/nYQN/pNsP//sXBfsTELa1dVlrLEz90d6eror+/Ib7lAAAAAAcIyGAgAAAIBjNBQAAAAAHKOhAAAAAOAYDQUAAAAAx2goAAAAADhGQwEAAADAMRoKAAAAAI4xsR0AAAliZ2K748ePG2vsTJQ1ePBgY42dSbkAL+no6DDWVFVVGWseeughY80999xjrNm2bZux5tSpU8Yav+EOBQAAAADHaCgAAAAAOEZDAQAAAMAxGgoAAAAAjtFQAAAAAHCMhgIAAACAYzQUAAAAAByjoQAAAADgGBPbAQCQIIFAwFgzYID5Up2WZv7/QcuybI3JxM6Y3doX4IbOzk5XtjNw4EBXtpOMuEMBAAAAwDEaCgAAAACO0VAAAAAAcIyGAgAAAIBjNBQAAAAAHKOhAAAAAOAYDQUAAAAAx2goAAAAADjGxHYAfKO7u9tY09zcbKxpaWkx1jAxF+LBzjl94MABY80777xjrBkzZoyxZtq0acaampoaYw35gZfYOR/tXBcuX77sxnCSUp/vULz11ltasGCBQqGQAoGAduzY0ePxxYsXKxAI9Fjmz5/v1ngBzyIbQGRkA4iOfCAZ9LmhaG1t1bRp01RZWRm1Zv78+Tp37lx42bZtW78GCfgB2QAiIxtAdOQDyaDPf/JUUlKikpKSXmuCwaDy8vIcDwrwI7IBREY2gOjIB5JBTN6UXVVVpZEjR+r222/Xww8/rMbGxqi17e3tampq6rEAyaov2ZDIB1IH2QCiIx/wOtcbivnz52vr1q3atWuXnnrqKf3zn/9USUmJurq6ItaXl5crOzs7vNh50xjgR33NhkQ+kBrIBhAd+YAfuP4pTw888ED431OnTtWdd96pW265RVVVVZo3b94N9StXrtTy5cvDXzc1NXHiIyn1NRsS+UBqIBtAdOQDfhDzeSgmTpyo4cOH68SJExEfDwaDGjZsWI8FSAWmbEjkA6mJbADRkQ94UcznoThz5owaGxs1atQoW/V8djXc4IfzqK/ZkPzxvGLJzmf2t7W1GWva29td2Zcf+eEcIhs99fanLde1trYaa+zM0XL16lVjTbIea788L/LRd52dncYaO/NQ2MlZql47+txQtLS09OiKa2trdfjwYeXk5CgnJ0fr1q3Tt771LeXl5enkyZNasWKFbr31VhUXF9vavp0feIBJc3OzsrOz47rPWGdDIh+nTp0y1qxatSr2A/ExsuE/hw4dMtYwL0H/JSIbEvmIh4MHDxprZs6cGYeR+JcpHwGrj21rVVWV5s6de8P6RYsW6dlnn1VpaakOHTqkixcvKhQK6d5779XPfvYz5ebm2tp+d3e3zp49q6ysLAUCAUn//du/uro6btvFSLIcY8uy1NzcrFAopLS0mP9FXw+xzoZ0Yz6S5XXzumQ4zqmWDSk5XjevS4ZjnMhsSFw7klkyHGe7+ehzQ5EITU1Nys7O1qVLl3z7gngdx9ifeN3ig+PsT7xusccx9idet/hIpeMc/1YcAAAAQNKgoQAAAADgmC8aimAwqLKyMgWDwUQPJWlxjP2J1y0+OM7+xOsWexxjf+J1i49UOs6+eA8FAAAAAG/yxR0KAAAAAN5EQwEAAADAMRoKAAAAAI7RUAAAAABwzPMNRWVlpcaPH69BgwapoKBA+/btS/SQfO2tt97SggULFAqFFAgEtGPHjh6PW5alNWvWaNSoUcrMzFRRUZFqamoSM1gYkQ/3kI3kQjbcRT6SC/lwD9m4xtMNxcsvv6zly5errKxMb7/9tqZNm6bi4mJ99NFHiR6ab7W2tmratGmqrKyM+PjGjRu1adMmbd68WdXV1RoyZIiKi4vV1tYW55HChHy4i2wkD7LhPvKRPMiHu8jG/7M8LD8/31q2bFn4666uLisUClnl5eUJHFXykGRt3749/HV3d7eVl5dn/fznPw+vu3jxohUMBq1t27YlYIToDfmIHbLhb2QjtsiHv5GP2EnlbHj2DkVHR4cOHjyooqKi8Lq0tDQVFRVpz549CRxZ8qqtrVV9fX2PY56dna2CggKOuceQj/giG/5BNuKPfPgH+YivVMqGZxuKCxcuqKurS7m5uT3W5+bmqr6+PkGjSm7XjyvH3PvIR3yRDf8gG/FHPvyDfMRXKmXDsw0FAAAAAO/zbEMxfPhwpaenq6Ghocf6hoYG5eXlJWhUye36ceWYex/5iC+y4R9kI/7Ih3+Qj/hKpWx4tqHIyMjQ9OnTtWvXrvC67u5u7dq1SzNnzkzgyJLXhAkTlJeX1+OYNzU1qbq6mmPuMeQjvsiGf5CN+CMf/kE+4iuVsjEg0QPozfLly7Vo0SLNmDFD+fn5qqioUGtrq5YsWZLooflWS0uLTpw4Ef66trZWhw8fVk5OjsaOHavHHntMGzZs0KRJkzRhwgStXr1aoVBIpaWliRs0IiIf7iIbyYNsuI98JA/y4S6y8f8S/TFTJr/5zW+ssWPHWhkZGVZ+fr61d+/eRA/J13bv3m1JumFZtGiRZVnXPuJs9erVVm5urhUMBq158+ZZ77//fmIHjajIh3vIRnIhG+4iH8mFfLiHbFwTsCzLim8LAwAAACBZePY9FAAAAAC8j4YCAAAAgGM0FAAAAAAco6EAAAAA4BgNBQAAAADHaCgAAAAAOEZDAQAAAMAxGgoAAAAAjtFQAAAAAHCMhgIAAACAYzQUAAAAAByjoQAAAADg2P8BXn3UwxAMFBMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"VISUALIZING THE DATA SET\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##step 1\n",
    "def visualize_data(x, y, pred=None):\n",
    "    # Step 2\n",
    "    n = len(x)\n",
    "    \n",
    "    # Step 3\n",
    "    labels_list = [\"\\u2212\", \"\\u002b\", \"\\ua714\", \"\\u02e7\"]\n",
    "    \n",
    "    # Step 4\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(8, 2))  \n",
    "    # Step 5\n",
    "    for i in range(n):\n",
    "        # Step 6\n",
    "        axes[i].imshow(x[i], cmap='gray')\n",
    "        \n",
    "        # Step 7\n",
    "        if pred is None:\n",
    "            axes[i].set_title('Label: {}'.format(labels_list[y[i]]))  \n",
    "        else:\n",
    "            axes[i].set_title('Label: {}, Pred: {}'.format(labels_list[y[i]], labels_list[pred[i]])) \n",
    "    \n",
    "    # Step 8\n",
    "    plt.tight_layout(w_pad=2)\n",
    "    plt.show()  # Added plt.show() to display the figure\n",
    "\n",
    "# Example usage\n",
    "visualize_data(x_vis, y_vis)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1\n",
    "#1) 16x16 pixels\n",
    "#2) the overall prection outcome will be predicted amongst the 4 classes \n",
    "(the 4 random classes that we have assinged)\n",
    "#3) there will be 32 layers. More layers means more complex calculations\n",
    "#4) ther will be 8 input qubits\n",
    "#5) the number of times the data will be reuploaded so as in to rotate from classical data to quantum data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING THE QML CLASSIFIER CIRCUIT\n"
     ]
    }
   ],
   "source": [
    "print(\"BUILDING THE QML CLASSIFIER CIRCUIT\")\n",
    "\n",
    "#step 1\n",
    "input_dim = 256\n",
    "num_classes = 4\n",
    "num_layers = 32\n",
    "num_qubits = 8\n",
    "num_reup = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while my GPU CUDA is installing, I will for now use CPU while I am aware that the running process will be slow, I will still use CPU just for learning purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "#step 2\n",
    "import torch\n",
    "comp_processor = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device\", comp_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we create a function called QML classifier because now we will be building a Quantum Classifier Model with our input torch.nn.Module\n",
    "\n",
    "few arguements to be assgined\n",
    "1) input_dim: dimensions of input samples\n",
    "2) output_dim: number of output dimensions: typically assigned as the number of classes\n",
    "3) num_qubits: number of qubits in a circuit\n",
    "4) num_layers: number of layers within the strongly entanglling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#step 4: in step 4, first we create the def __init __function with our assigned attributes as self, input_dimensions (the dimension of the input samples), output_dim: (the dimension of the output, i.e. the numbers of classes), num_qubits (the number of qubits in the circuit), num_layers (the number of layers within the StronglyEntanglingLayers template). Here on, we also use a super init function where we include everything so this is why we arent segregating anything from def init function and super init funciton. torch.manual_seed(13337) this means that even if we were to run the code 13337 times, (obviously no one would but just to be safe), the overall output would remain the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qml.qnode(self.device) this is like a quantum node (just like a quantum pointer, that will link between the quantum circuit and the pytorch layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#step 5: when building our quantum circuit, few key attributes that are in place are the inputs, typical input datas, weights (parameters or theta that control the quantum gate and bias, creates a shift or tweaks a little to a more favourable side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#step 6: torch.reshape the reason we do that is to resize or reshape (maybe crops the image deacreases pixel whatever) so that the input data fits into the quanutm circuit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#step 7: equation for the regular neural network is weight = weight * input + bias where we multiply the weight value with the input value and tweak a little so as to favour towards one point by adding with bias value. With all those with wires ranging from 0 to num_qubits we add a \"StronglyEntanglingLayers\" so as to add trhose layers, meaning more mathematical copmlex formulas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#step 8: here for each of the output dimenstions, (so typically for each class) we will measure the average value in regards to pauli z operator because pauli Z consists of bit values 0 or 1 so it will typically collapse multiple solutions into either ket 0 or ket 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QML_Classifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Step 1: Initialization of the QML_Classifier class\n",
    "    Args:\n",
    "        input_dim: the dimension of the input samples\n",
    "        output_dim: the dimension of the output, i.e. the number of classes\n",
    "        num_qubits: the number of qubits in the circuit\n",
    "        num_layers: the number of layers within the StronglyEntanglingLayers template\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 2: Constructor (init) method\n",
    "    def __init__(self, input_dim, output_dim, num_qubits, num_layers):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1337)  # Set seed for reproducibility\n",
    "        \n",
    "        # Step 3: Set class attributes\n",
    "        self.output_dim = output_dim\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Step 4: Define the quantum device\n",
    "        self.device = qml.device(\"lightning.qubit\", wires=self.num_qubits)\n",
    "\n",
    "        # Step 5: Define the shape of the weights for the Strongly Entangling Layers\n",
    "        self.weights_shape = qml.StronglyEntanglingLayers.shape(\n",
    "            n_layers=self.num_layers, n_wires=self.num_qubits\n",
    "        )\n",
    "\n",
    "        # Step 6: Define the quantum circuit\n",
    "        @qml.qnode(self.device)\n",
    "        def circuit(inputs, weights, bias):\n",
    "            # Step 7: Reshape the inputs to match the shape of the weights\n",
    "            inputs = torch.reshape(inputs, self.weights_shape)\n",
    "            \n",
    "            # Step 8: Apply the Strongly Entangling Layers with input transformation and bias\n",
    "            qml.StronglyEntanglingLayers(\n",
    "                weights=weights * (inputs + bias), wires=range(self.num_qubits)\n",
    "            )\n",
    "            \n",
    "            # Step 9: Measure the Pauli-Z expectation values for each qubit\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.output_dim)]\n",
    "\n",
    "        # Step 10: Define the parameter shapes and initialize the parameters\n",
    "        param_shapes = {\"weights\": self.weights_shape, \"bias\": self.weights_shape}\n",
    "        init_vals = {\n",
    "            \"weights\": 0.1 * torch.rand(self.weights_shape),\n",
    "            \"bias\": 0.1 * torch.rand(self.weights_shape),\n",
    "        }\n",
    "\n",
    "        # Step 11: Initialize the quantum circuit with TorchLayer\n",
    "        self.qcircuit = qml.qnn.TorchLayer(\n",
    "            qnode=circuit, weight_shapes=param_shapes, init_method=init_vals\n",
    "        )\n",
    "\n",
    "    # Step 12: Define the forward pass for the model\n",
    "    def forward(self, x):\n",
    "        # Step 13: Re-upload inputs multiple times by stacking the input\n",
    "        inputs_stack = torch.hstack([x] * 3)  # Re-upload the inputs 3 times\n",
    "        \n",
    "        # Step 14: Pass the stacked inputs to the quantum circuit\n",
    "        return self.qcircuit(inputs_stack)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hyperparameters and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the learning_rate [0.1], epochs [4], and batch_szie [20] are relatively low values but only for learning purposes pennylane has assigned those values. However, the values are relativewly hgiher but given, even in my personal point of view, I would liek to compute it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1\n",
    "learning_rate = 0.1\n",
    "epochs = 4\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "feats_train = torch.from_numpy(X_train[:200]).reshape(200, -1).to(comp_processor)\n",
    "feats_test = torch.from_numpy(X_test[:50]).reshape(50, -1).to(comp_processor)\n",
    "label_train = torch.from_numpy(Y_train[:200]).to(comp_processor)\n",
    "label_test = torch.from_numpy(Y_test[:50]).to(comp_processor)\n",
    "num_train = feats_train.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CWQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
